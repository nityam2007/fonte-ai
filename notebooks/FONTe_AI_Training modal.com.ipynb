{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0aa2c47",
   "metadata": {},
   "source": [
    "# üé® FONTe AI - B200 Training (Modal.com)\n",
    "\n",
    "**Run this notebook directly on Modal.com**\n",
    "\n",
    "GPU: B200 (192GB) @ $6.25/hr | ~50 epochs in ~1.3 hours | ~$8 total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d374cfd",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43857891",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install git-lfs -qq\n",
    "!git lfs install\n",
    "!git clone https://github.com/nityam2007/fonte-ai.git\n",
    "%cd fonte-ai\n",
    "!git lfs pull\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
    "!ls -lh TOKENIZED/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb2b06",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import struct\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Config\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 50\n",
    "LR = 3e-4\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = 1106  # FIXED: was 1105, now includes <NEG> token at ID 24\n",
    "    max_seq_length: int = 512\n",
    "    d_model: int = 256\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 6\n",
    "    d_ff: int = 1024\n",
    "    dropout: float = 0.1\n",
    "    pad_token_id: int = 0\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads, self.d_k = n_heads, d_model // n_heads\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, D = x.shape\n",
    "        q = self.wq(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.wk(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.wv(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = self.dropout(F.softmax(scores, dim=-1))\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, L, -1)\n",
    "        return self.wo(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model))\n",
    "        self.n1 = nn.LayerNorm(d_model)\n",
    "        self.n2 = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.drop(self.attn(self.n1(x), mask))\n",
    "        return x + self.drop(self.ff(self.n2(x)))\n",
    "\n",
    "class FonteModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.emb = nn.Embedding(cfg.vocab_size, cfg.d_model, padding_idx=cfg.pad_token_id)\n",
    "        self.pos = PositionalEncoding(cfg.d_model, cfg.max_seq_length, cfg.dropout)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg.d_model, cfg.n_heads, cfg.d_ff, cfg.dropout) for _ in range(cfg.n_layers)])\n",
    "        self.norm = nn.LayerNorm(cfg.d_model)\n",
    "        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "        self.head.weight = self.emb.weight\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(cfg.max_seq_length, cfg.max_seq_length)))\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        x = self.pos(self.emb(input_ids))\n",
    "        m = self.mask[:x.size(1), :x.size(1)]\n",
    "        for b in self.blocks:\n",
    "            x = b(x, m)\n",
    "        logits = self.head(self.norm(x))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits[:, :-1].reshape(-1, self.cfg.vocab_size), labels[:, 1:].reshape(-1), ignore_index=self.cfg.pad_token_id)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "class BinaryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Load binary dataset with proper format:\n",
    "    - Header: 12 bytes (num_sequences, max_len, vocab_size as uint32)\n",
    "    - Per sequence: 2 bytes (length as uint16) + max_len*2 bytes (tokens as uint16)\n",
    "    \"\"\"\n",
    "    def __init__(self, bin_path, json_path=None, max_len=512):\n",
    "        # Read binary file header and data\n",
    "        with open(bin_path, 'rb') as f:\n",
    "            # Read 12-byte header\n",
    "            header = f.read(12)\n",
    "            self.count, self.max_len, self.vocab_size = struct.unpack('III', header)\n",
    "            # Read rest of file\n",
    "            self.data = f.read()\n",
    "        \n",
    "        # Calculate bytes per sequence: 2 (length) + max_len * 2 (tokens)\n",
    "        self.bytes_per_seq = 2 + self.max_len * 2\n",
    "        \n",
    "        print(f\"Loaded {self.count:,} sequences, max_len={self.max_len}, vocab_size={self.vocab_size}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate offset (each sequence: 2 byte length + max_len * 2 byte tokens)\n",
    "        offset = idx * self.bytes_per_seq\n",
    "        \n",
    "        # Read length (2 bytes) - we don't actually use it but need to skip it\n",
    "        # length = struct.unpack('H', self.data[offset:offset+2])[0]\n",
    "        \n",
    "        # Read tokens starting after length field\n",
    "        token_offset = offset + 2\n",
    "        tokens = struct.unpack(f'{self.max_len}H', self.data[token_offset:token_offset + self.max_len * 2])\n",
    "        \n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "app = modal.App(\"fonte-ai-training\")\n",
    "vol = modal.Volume.from_name(\"fonte-data\", create_if_missing=True)\n",
    "\n",
    "image = modal.Image.debian_slim(python_version=\"3.11\").pip_install(\"torch\", \"tqdm\")\n",
    "\n",
    "@app.function(image=image, gpu=\"B200\", timeout=7200, volumes={\"/data\": vol})\n",
    "def train():\n",
    "    train_ds = BinaryDataset(\"/data/train.bin\")\n",
    "    val_ds = BinaryDataset(\"/data/val.bin\")\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, num_workers=4)\n",
    "    \n",
    "    cfg = ModelConfig()\n",
    "    model = FonteModel(cfg).to(DEVICE)\n",
    "    print(f\"Model: {model.count_params()/1e6:.1f}M params, vocab_size={cfg.vocab_size}\")\n",
    "    \n",
    "    opt = AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "    sched = CosineAnnealingLR(opt, T_max=EPOCHS * len(train_loader))\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"E{epoch+1}\"):\n",
    "            batch = batch.to(DEVICE)\n",
    "            out = model(batch, batch)\n",
    "            loss = out['loss']\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                val_loss += model(batch, batch)['loss'].item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: train_loss={total_loss/len(train_loader):.4f}, val_loss={val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save({'config': cfg.__dict__, 'state_dict': model.state_dict()}, f\"/data/best_model.pt\")\n",
    "        torch.save({'config': cfg.__dict__, 'state_dict': model.state_dict()}, f\"/data/epoch_{epoch+1}.pt\")\n",
    "    \n",
    "    vol.commit()\n",
    "    return {\"final_val_loss\": val_loss, \"best_val_loss\": best_loss}\n",
    "\n",
    "@app.local_entrypoint()\n",
    "def main():\n",
    "    result = train.remote()\n",
    "    print(f\"Training complete: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a37051",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93625ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all checkpoints\n",
    "!ls -lh TRAINED/\n",
    "\n",
    "# If on Modal, download via modal volume or use:\n",
    "# !zip -r trained_models.zip TRAINED/\n",
    "# Then download trained_models.zip from Modal UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
