{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0aa2c47",
   "metadata": {},
   "source": [
    "# ðŸŽ¨ FONTe AI - Font Generation Training\n",
    "\n",
    "Train an AI model to generate unique fonts using SVG path transformers.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab (Free tier works!)\n",
    "- T4 GPU (16GB) - plenty for this model\n",
    "- ~6-10 hours for full training\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Setup environment\n",
    "2. Upload/download training data\n",
    "3. Train the model\n",
    "4. Generate sample fonts\n",
    "5. Export model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d374cfd",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43857891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3670d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for saving checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory\n",
    "!mkdir -p /content/fonte_ai\n",
    "%cd /content/fonte_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed4d89",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Upload Training Data\n",
    "\n",
    "Upload your `TOKENIZED` folder from local machine, or clone from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a25a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Upload from local (run this cell, then drag & drop files)\n",
    "from google.colab import files\n",
    "\n",
    "# Upload train.bin, val.bin, vocabulary.json\n",
    "print(\"Upload your training files (train.bin, val.bin, vocabulary.json):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move to proper location\n",
    "!mkdir -p TOKENIZED\n",
    "!mv train.bin val.bin vocabulary.json TOKENIZED/ 2>/dev/null || true\n",
    "!ls -la TOKENIZED/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3456ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Copy from Google Drive\n",
    "# (If you've uploaded the TOKENIZED folder to Drive)\n",
    "# !cp -r /content/drive/MyDrive/fonte_ai/TOKENIZED ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb2b06",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import struct\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = 1105\n",
    "    max_seq_length: int = 512\n",
    "    d_model: int = 256\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 6\n",
    "    d_ff: int = 1024\n",
    "    dropout: float = 0.1\n",
    "    pad_token_id: int = 0\n",
    "    sos_token_id: int = 1\n",
    "    eos_token_id: int = 2\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.w_o(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.attention(self.norm1(x), mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class FonteModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model, padding_idx=config.pad_token_id)\n",
    "        self.pos_encoding = PositionalEncoding(config.d_model, config.max_seq_length, config.dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(config.d_model, config.n_heads, config.d_ff, config.dropout)\n",
    "            for _ in range(config.n_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(config.d_model)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "        self.apply(self._init_weights)\n",
    "        self.register_buffer('causal_mask', torch.tril(torch.ones(config.max_seq_length, config.max_seq_length)))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.pos_encoding(x)\n",
    "        mask = self.causal_mask[:seq_len, :seq_len]\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        result = {'logits': logits}\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            loss = F.cross_entropy(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1), ignore_index=self.config.pad_token_id)\n",
    "            result['loss'] = loss\n",
    "        return result\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, style_id, char_id, max_length=256, temperature=1.0, top_k=50, top_p=0.9):\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "        tokens = torch.tensor([[self.config.sos_token_id, style_id, char_id]], device=device)\n",
    "        for _ in range(max_length - 3):\n",
    "            outputs = self.forward(tokens)\n",
    "            logits = outputs['logits'][:, -1, :] / temperature\n",
    "            if top_k > 0:\n",
    "                indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "            if next_token.item() == self.config.eos_token_id:\n",
    "                break\n",
    "        return tokens\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({'config': self.config.__dict__, 'state_dict': self.state_dict()}, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path, device='cpu'):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        config = ModelConfig(**checkpoint['config'])\n",
    "        model = cls(config)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        return model\n",
    "\n",
    "\n",
    "print(\"âœ… Model classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a37051",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93625ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FonteDataset(Dataset):\n",
    "    def __init__(self, data_path, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        with open(data_path, 'rb') as f:\n",
    "            num_sequences, max_length, vocab_size = struct.unpack('III', f.read(12))\n",
    "            self.token_ids = []\n",
    "            self.lengths = []\n",
    "            for _ in range(num_sequences):\n",
    "                length = struct.unpack('H', f.read(2))[0]\n",
    "                tokens = list(struct.unpack(f'{max_length}H', f.read(max_length * 2)))\n",
    "                self.lengths.append(length)\n",
    "                self.token_ids.append(tokens)\n",
    "        print(f\"Loaded {len(self.token_ids)} sequences\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.token_ids[idx], dtype=torch.long),\n",
    "            'length': self.lengths[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = FonteDataset('TOKENIZED/train.bin')\n",
    "val_dataset = FonteDataset('TOKENIZED/val.bin')\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d295b",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25740497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "BATCH_SIZE = 64          # Increase for faster training on T4\n",
    "EPOCHS = 50              # 50-100 for good results\n",
    "LEARNING_RATE = 3e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Model config (medium size ~12M params)\n",
    "config = ModelConfig(\n",
    "    vocab_size=1105,\n",
    "    max_seq_length=512,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    n_layers=6,\n",
    "    d_ff=1024,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = FonteModel(config).to(DEVICE)\n",
    "print(f\"Model parameters: {model.count_parameters():,}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader) * EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e6c4b",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs['loss']\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        total_loss += outputs['loss'].item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Training\n",
    "best_val_loss = float('inf')\n",
    "history = []\n",
    "\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, DEVICE)\n",
    "    val_loss = validate(model, val_loader, DEVICE)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    history.append({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss})\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model.save('best_model.pt')\n",
    "        print(f\"  ðŸ’¾ Saved best model!\")\n",
    "    \n",
    "    # Save to Drive every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        model.save(f'/content/drive/MyDrive/fonte_ai/checkpoint_epoch_{epoch}.pt')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nâœ… Training complete in {total_time/60:.1f} minutes!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0654e1",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb70bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = FonteModel.load('best_model.pt', device=DEVICE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Style and character token mappings\n",
    "STYLE_IDS = {\n",
    "    'serif': 28,\n",
    "    'sans-serif': 29,\n",
    "    'monospace': 30,\n",
    "    'handwriting': 31,\n",
    "    'display': 32,\n",
    "}\n",
    "\n",
    "CHAR_IDS = {char: 33 + i for i, char in enumerate(\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789!@#$%&*()-+=[]\")}\n",
    "\n",
    "# Generate a sample\n",
    "style = 'serif'\n",
    "char = 'A'\n",
    "\n",
    "tokens = model.generate(\n",
    "    style_id=STYLE_IDS[style],\n",
    "    char_id=CHAR_IDS[char],\n",
    "    max_length=256,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "print(f\"Generated {style} '{char}':\")\n",
    "print(f\"Tokens: {tokens[0].tolist()[:30]}...\")\n",
    "print(f\"Total tokens: {tokens.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7606e48",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69153232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Drive\n",
    "!mkdir -p /content/drive/MyDrive/fonte_ai\n",
    "\n",
    "model.save('/content/drive/MyDrive/fonte_ai/final_model.pt')\n",
    "print(\"âœ… Model saved to Google Drive!\")\n",
    "\n",
    "# Save training history\n",
    "with open('/content/drive/MyDrive/fonte_ai/training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "# Download to local\n",
    "from google.colab import files\n",
    "files.download('best_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4343bd8c",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Done!\n",
    "\n",
    "Your model is trained! Next steps:\n",
    "1. Download `best_model.pt`\n",
    "2. Use the generation script to create fonts\n",
    "3. Convert to TTF using svgtofont"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
