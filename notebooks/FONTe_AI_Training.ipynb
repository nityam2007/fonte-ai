{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0aa2c47",
   "metadata": {},
   "source": [
    "# ðŸŽ¨ FONTe AI - Font Generation Training\n",
    "\n",
    "**AI-powered unique font generation using SVG Path Transformers**\n",
    "\n",
    "Repository: [github.com/nityam2007/fonte-ai](https://github.com/nityam2007/fonte-ai)\n",
    "\n",
    "---\n",
    "\n",
    "## What this notebook does:\n",
    "1. âœ… Clone repository with training data (Git LFS)\n",
    "2. âœ… Setup environment\n",
    "3. âœ… Load pre-tokenized dataset (248K sequences)\n",
    "4. âœ… Train SVG Path Transformer model\n",
    "5. âœ… Generate sample fonts\n",
    "6. âœ… Save model to Drive\n",
    "\n",
    "**Requirements:** Google Colab (Free T4 GPU works!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d374cfd",
   "metadata": {},
   "source": [
    "---\n",
    "## 1ï¸âƒ£ Setup & Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43857891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3670d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for saving checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project folder in Drive\n",
    "!mkdir -p /content/drive/MyDrive/fonte_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a25a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Git LFS and clone repository\n",
    "!apt-get install git-lfs -qq\n",
    "!git lfs install\n",
    "\n",
    "# Clone the repository (includes LFS files)\n",
    "%cd /content\n",
    "!git clone https://github.com/nityam2007/fonte-ai.git\n",
    "%cd fonte-ai\n",
    "\n",
    "# Pull LFS files\n",
    "!git lfs pull\n",
    "\n",
    "# Check data files\n",
    "!ls -lh TOKENIZED/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb2b06",
   "metadata": {},
   "source": [
    "---\n",
    "## 2ï¸âƒ£ Model Architecture\n",
    "\n",
    "SVG Path Transformer - treats font glyphs as sequences of path commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import struct\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = 1105\n",
    "    max_seq_length: int = 512\n",
    "    d_model: int = 256\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 6\n",
    "    d_ff: int = 1024\n",
    "    dropout: float = 0.1\n",
    "    pad_token_id: int = 0\n",
    "    sos_token_id: int = 1\n",
    "    eos_token_id: int = 2\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.w_o(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.attention(self.norm1(x), mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class FonteModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model, padding_idx=config.pad_token_id)\n",
    "        self.pos_encoding = PositionalEncoding(config.d_model, config.max_seq_length, config.dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(config.d_model, config.n_heads, config.d_ff, config.dropout)\n",
    "            for _ in range(config.n_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(config.d_model)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "        self.apply(self._init_weights)\n",
    "        self.register_buffer('causal_mask', torch.tril(torch.ones(config.max_seq_length, config.max_seq_length)))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.pos_encoding(x)\n",
    "        mask = self.causal_mask[:seq_len, :seq_len]\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        result = {'logits': logits}\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            loss = F.cross_entropy(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1), ignore_index=self.config.pad_token_id)\n",
    "            result['loss'] = loss\n",
    "        return result\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, style_id, char_id, max_length=256, temperature=1.0, top_k=50, top_p=0.9):\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "        tokens = torch.tensor([[self.config.sos_token_id, style_id, char_id]], device=device)\n",
    "        for _ in range(max_length - 3):\n",
    "            outputs = self.forward(tokens)\n",
    "            logits = outputs['logits'][:, -1, :] / temperature\n",
    "            if top_k > 0:\n",
    "                indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "            if next_token.item() == self.config.eos_token_id:\n",
    "                break\n",
    "        return tokens\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({'config': self.config.__dict__, 'state_dict': self.state_dict()}, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path, device='cpu'):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        config = ModelConfig(**checkpoint['config'])\n",
    "        model = cls(config)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        return model\n",
    "\n",
    "\n",
    "print(\"âœ… Model classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a37051",
   "metadata": {},
   "source": [
    "---\n",
    "## 3ï¸âƒ£ Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93625ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FonteDataset(Dataset):\n",
    "    def __init__(self, data_path, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        with open(data_path, 'rb') as f:\n",
    "            num_sequences, max_length, vocab_size = struct.unpack('III', f.read(12))\n",
    "            self.token_ids = []\n",
    "            self.lengths = []\n",
    "            for _ in range(num_sequences):\n",
    "                length = struct.unpack('H', f.read(2))[0]\n",
    "                tokens = list(struct.unpack(f'{max_length}H', f.read(max_length * 2)))\n",
    "                self.lengths.append(length)\n",
    "                self.token_ids.append(tokens)\n",
    "        print(f\"Loaded {len(self.token_ids)} sequences\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.token_ids[idx], dtype=torch.long),\n",
    "            'length': self.lengths[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = FonteDataset('TOKENIZED/train.bin')\n",
    "val_dataset = FonteDataset('TOKENIZED/val.bin')\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d295b",
   "metadata": {},
   "source": [
    "---\n",
    "## 4ï¸âƒ£ Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25740497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "BATCH_SIZE = 64          # Increase for faster training on T4\n",
    "EPOCHS = 50              # 50-100 for good results\n",
    "LEARNING_RATE = 3e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Model config (medium size ~12M params)\n",
    "config = ModelConfig(\n",
    "    vocab_size=1105,\n",
    "    max_seq_length=512,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    n_layers=6,\n",
    "    d_ff=1024,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = FonteModel(config).to(DEVICE)\n",
    "print(f\"Model parameters: {model.count_parameters():,}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader) * EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e6c4b",
   "metadata": {},
   "source": [
    "---\n",
    "## 5ï¸âƒ£ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs['loss']\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        total_loss += outputs['loss'].item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Training\n",
    "best_val_loss = float('inf')\n",
    "history = []\n",
    "\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, DEVICE)\n",
    "    val_loss = validate(model, val_loader, DEVICE)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    history.append({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss})\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model.save('best_model.pt')\n",
    "        print(f\"  ðŸ’¾ Saved best model!\")\n",
    "    \n",
    "    # Save to Drive every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        model.save(f'/content/drive/MyDrive/fonte_ai/checkpoint_epoch_{epoch}.pt')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nâœ… Training complete in {total_time/60:.1f} minutes!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0654e1",
   "metadata": {},
   "source": [
    "---\n",
    "## 6ï¸âƒ£ Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb70bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = FonteModel.load('best_model.pt', device=DEVICE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Style and character token mappings\n",
    "STYLE_IDS = {\n",
    "    'serif': 28,\n",
    "    'sans-serif': 29,\n",
    "    'monospace': 30,\n",
    "    'handwriting': 31,\n",
    "    'display': 32,\n",
    "}\n",
    "\n",
    "CHAR_IDS = {char: 33 + i for i, char in enumerate(\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789!@#$%&*()-+=[]\")}\n",
    "\n",
    "# Generate a sample\n",
    "style = 'serif'\n",
    "char = 'A'\n",
    "\n",
    "tokens = model.generate(\n",
    "    style_id=STYLE_IDS[style],\n",
    "    char_id=CHAR_IDS[char],\n",
    "    max_length=256,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "print(f\"Generated {style} '{char}':\")\n",
    "print(f\"Tokens: {tokens[0].tolist()[:30]}...\")\n",
    "print(f\"Total tokens: {tokens.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7606e48",
   "metadata": {},
   "source": [
    "---\n",
    "## 7ï¸âƒ£ Save Training History & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69153232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Drive\n",
    "!mkdir -p /content/drive/MyDrive/fonte_ai\n",
    "\n",
    "model.save('/content/drive/MyDrive/fonte_ai/final_model.pt')\n",
    "print(\"âœ… Model saved to Google Drive!\")\n",
    "\n",
    "# Save training history\n",
    "with open('/content/drive/MyDrive/fonte_ai/training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "# Download to local\n",
    "from google.colab import files\n",
    "files.download('best_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4343bd8c",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Done!\n",
    "\n",
    "Your model is trained! Next steps:\n",
    "1. Download `best_model.pt`\n",
    "2. Use the generation script to create fonts\n",
    "3. Convert to TTF using svgtofont"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
